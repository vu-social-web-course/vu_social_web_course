{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dcMf4aubeMI9"
   },
   "source": [
    "*****************************************************************\n",
    "#  The Social Web: data representation\n",
    "- Instructors: Davide Ceolin, Filip Ilievski and Zubaria Inayat.\n",
    "- TAs: MÃ¡rton BodÃ³, Manav Neema, GÃ¶ktuÄŸ Genckaya and Chiara Michelutti.\n",
    "- Exercises for Hands-on session 2\n",
    "*****************************************************************"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Zhts5HMzeMI-"
   },
   "source": [
    "In this session you are going to mine data in various microformats. You will see the differences in what each of the formats can contain and what purpose they serve. We will start by looking at geographical data.\n",
    "\n",
    "**Prerequisites:**\n",
    "- Python 3.8\n",
    "- Python packages: `requests`, `BeautifulSoup4`, `HTMLParser`, `rdflib`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "id": "6f-OtFPPeMJA",
    "outputId": "9bcb836f-4204-4fac-d133-99e81a0b2884"
   },
   "outputs": [],
   "source": [
    "# If you're using a virtualenv, make sure it's activated before running\n",
    "# this cell!\n",
    "!pip install requests\n",
    "!pip install BeautifulSoup4\n",
    "!pip install HTMLParser\n",
    "!pip install rdflib\n",
    "!pip install cloudscraper\n",
    "!pip install scrape-schema-recipe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "id": "6f-OtFPPeMJA",
    "outputId": "9bcb836f-4204-4fac-d133-99e81a0b2884"
   },
   "source": [
    "> ðŸ’¡ **Info:** If the you get `command not found: pip` try with `pip3`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "id": "6f-OtFPPeMJA",
    "outputId": "9bcb836f-4204-4fac-d133-99e81a0b2884"
   },
   "outputs": [],
   "source": [
    "# pip3 version \n",
    "# If you're using a virtualenv, make sure it's activated before running\n",
    "# this cell!\n",
    "!pip3 install requests\n",
    "!pip3 install BeautifulSoup4\n",
    "!pip3 install HTMLParser\n",
    "!pip3 install rdflib\n",
    "!pip3 install cloudscraper\n",
    "!pip3 install scrape-schema-recipe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "irPnmIK4eMJd"
   },
   "source": [
    "##  Exercise 1\n",
    "\n",
    "Even if web pages do not use microformat, interesting data can often be extracted from the HTML. You may use packages such as [BeautifulSoup][b] to extract arbitrary pieces of data from any HTML page.  \n",
    "The example below shows how we can find the URL of the first image in the infobox table of the [wikipedia page on Amsterdam][a]. \n",
    "\n",
    "**Tip:** compare the code below with HTML source code of the wikipedia page: the image url is in the `\"src\"` attribute of the `\"img\"` element of in the `\"table\"` element with `class=\"infobox\"`.\n",
    "\n",
    "[b]: https://beautiful-soup-4.readthedocs.io/en/latest/\n",
    "[a]: https://en.wikipedia.org/wiki/Amsterdam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "9gpHw90keMJf",
    "outputId": "7ae1fe64-8d85-4a47-cfdf-422284954d81"
   },
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# This script requires you to add a url of a page with geotags to the commandline, e.g.\n",
    "# python geo.py 'http://en.wikipedia.org/wiki/Amsterdam'\n",
    "URL = 'https://en.wikipedia.org/wiki/Amsterdam'\n",
    "\n",
    "req = requests.get(URL, headers={'User-Agent' : \"Social Web Course Student\"})\n",
    "soup = BeautifulSoup(req.text)\n",
    "#print(req.text)\n",
    "image1 = soup.find_all('table', class_='infobox')[0].find('img')\n",
    "print(image1['src'])\n",
    "print('https:' + image1['src'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "9gpHw90keMJf",
    "outputId": "7ae1fe64-8d85-4a47-cfdf-422284954d81"
   },
   "source": [
    "> ðŸ’¡ **Info:** If you are on a Mac and you see an issue with the ssl version, try running `brew install openssl@1.1`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at another example:\n",
    "\n",
    "The code bellow extracts the coordinates from a webpage and reformats them into geo microformat (based on Example 8-1 in Mining the Social Web). **Note** that wikipages may encode long/lat information in different ways.   \n",
    "One of the ways used by the Amsterdam wikipedia page is in a `span` element that is **not** shown to the user:\n",
    "```html\n",
    "<span class=\"geo\">52.367; 4.900</span>\n",
    "```\n",
    "This `span` element has a single child: \n",
    "```html\n",
    "len(geoTag == 1) \n",
    "```\n",
    "and no further structure. Therefore, we have to manually get the long/lat by splitting the string on the `';'` semicolon."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "LtHtQT9PeMJl",
    "outputId": "8a7f7b52-cdb2-409f-b3f0-ee7adf60a9f7"
   },
   "outputs": [],
   "source": [
    "geoTag = soup.find(True, 'geo')\n",
    "print(geoTag)\n",
    "\n",
    "if geoTag and len(geoTag) > 1:\n",
    "        lat = geoTag.find(True, 'latitude').string\n",
    "        lon = geoTag.find(True, 'longitude').string\n",
    "        print ('Location is at'), lat, lon\n",
    "elif geoTag and len(geoTag) == 1:\n",
    "        (lat, lon) = geoTag.string.split(';')\n",
    "        (lat, lon) = (lat.strip(), lon.strip())\n",
    "        print (('Location is at'), lat, lon)\n",
    "else:\n",
    "        print ('Location not found')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8S_bXnjveMJp"
   },
   "source": [
    "### Task 1: From Scraped Coordinates to a World Map\n",
    "\n",
    "Your goal in this task is to take the latitude and longitude coordinates you scraped in Exercise 1 and plot that location on a map. You will do this by first creating a KML (Keyhole Markup Language) file.\n",
    "\n",
    "---\n",
    "\n",
    "#### Task 1.1 Convert the output of Exercise 1 into KML\n",
    "\n",
    "Your first step is to create a `.kml` file. This is just a plain text file, structured using XML (similar to HTML), that stores geographic data.\n",
    "\n",
    "To complete this step, you will need to:\n",
    "1.  Run your Python script from Exercise 1 to get the `latitude` and `longitude` values.\n",
    "2.  Create a new, empty text file and save it as `amsterdam.kml`.\n",
    "3.  Look at the documentation below to figure out the basic KML structure. You will need to create a \"Placemark\" for your location.\n",
    "\n",
    "**Hints:**\n",
    "* The most important tags you'll need to research are `<Placemark>`, `<Point>`, and `<coordinates>`.\n",
    "* KML requires coordinates in the format `longitude,latitude,altitude`. Your Python script outputs `latitude` *first*. Keep that in mind.\n",
    "\n",
    "**Documentation:**\n",
    "* **Official KML Documentation:** https://developers.google.com/kml/documentation/?csw=1\n",
    "* **Simple KML Example:** https://renenyffenegger.ch/notes/tools/Google-Earth/kml/index\n",
    "\n",
    "---\n",
    "#### Task 1.2 Visualize the point on a Map\n",
    "\n",
    "Once you have your `amsterdam.kml` file, you need to visualize it. There are two ways to do this: a simple way to check your work, and a more advanced way that uses JavaScript.\n",
    "\n",
    "**Option 1: The Simple Way (Used for Checking)**\n",
    "\n",
    "The fastest way to see if your KML file works is to use a free online viewer.\n",
    "\n",
    "- Go to a site like **https://kmzview.com/**.\n",
    "- Load `amsterdam.kml` file.\n",
    "- The map should fly to Amsterdam and show the placemark you defined. If it does, your KML file is correct!\n",
    "\n",
    "**Option 2: The Right Way (Using the Google Maps API)**\n",
    "\n",
    "This method involves building a custom HTML page that loads the Google Maps API and adds your KML file as a layer.\n",
    "\n",
    "This is a multi-step process. You will have to figure out how to:\n",
    "\n",
    "1.  **Host Your KML File:** Your KML file must be available on a public URL. You can use a service like **http://pastebin.com/** for this. After you paste your KML code, be sure to find and copy the **\"RAW\" format URL**.\n",
    "2.  **Get an API Key:** You cannot use the Google Maps API without an API key. You can get one from the **[Google Cloud Console](https://developers.google.com/maps/documentation/javascript/get-api-key)**.\n",
    "3.  **Write the Code:** Create an `index.html` file. You will need to write JavaScript that:\n",
    "    * Loads the Google Maps JavaScript API (using your new API key).\n",
    "    * Initializes a new map.\n",
    "    * Creates a `google.maps.KmlLayer` object, passing in the **\"RAW\" URL** of your pastebin file.\n",
    "    * Adds that KML layer to your map.\n",
    "\n",
    "* **Hint:** The official Google Maps documentation for adding a **[KML Layer](https://developers.google.com/maps/documentation/javascript/kml)** will be your most important resource for this step."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task 1.3\n",
    "Enter the link to your pin below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Link:** _Enter your url here_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task 1.4 \n",
    "Is KML a microformat, why (not)?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer:** (add your answer here)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kUnka7EyeMJp"
   },
   "source": [
    "## Exercise 2 \n",
    "In order to find information in the web, we can use microformats such as [hRecipe](https://microformats.org/wiki/hrecipe) or Schema.org's [Recipe](https://schema.org/Recipe). But first, we'll show you how to find arbitrary tags in a webpage."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b0pBs-PVeMJq"
   },
   "source": [
    "### Task 2 \n",
    "Parsing data for a <sub><sup>veggie</sup></sub> spaghetti alla carbonara recipe (from Example 2-7 in Mining the Social Web)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import cloudscraper\n",
    "import json\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# A yummy webpage (feel free to change to your likings.)\n",
    "URL = \"https://www.acouplecooks.com/spring-vegetarian-spaghetti-carbonara\"\n",
    "\n",
    "# Create a CloudScraper object\n",
    "scraper = cloudscraper.create_scraper()\n",
    "\n",
    "# Use the CloudScraper object to fetch the HTML content\n",
    "response = scraper.get(URL)\n",
    "\n",
    "# Parse the HTML content with BeautifulSoup\n",
    "soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "# Now you can work with the 'soup' object as you did before\n",
    "listchildren = list(soup.children)\n",
    "#print(listchildren)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IhdMwqykeMJt"
   },
   "source": [
    "We can find any element in the page through **css tag selectors** (you can find them all [here](https://www.w3schools.com/cssref/css_selectors.asp))\n",
    "\n",
    "But in short, these are:\n",
    "- `\".\"` for classes\n",
    "- `#` for ids \n",
    "- and `plain text` for the element name\n",
    "\n",
    "You can also combine them, so for example, looking for `\".class1.class2\"` would select all elements displaying both classes. For a deeper overview, please check the above link (or google \"html tag selectors\"). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(len(listchildren)) # we can see here how many children the html doc has got.\n",
    "title_unparsed = soup.select_one(\"title\")\n",
    "#show the title element\n",
    "print(title_unparsed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not so pretty.... Use the `.text` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(title_unparsed.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The website has a block of `JSON-LD` data embedded. Try to see if you can find it in the soup object. We can load the `JSON-LD` script to work with it easier.  \n",
    "Let's get a list of the ingredients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Find the script tag containing the JSON-LD data\n",
    "json_ld_script = soup.find(\"script\", {\"class\": \"yoast-schema-graph\"})\n",
    "\n",
    "# Extract the content of the script tag\n",
    "script_content = json_ld_script.string\n",
    "\n",
    "# Load the JSON data from the script content\n",
    "data = json.loads(script_content)\n",
    "\n",
    "# Access the \"recipeIngredient\" list\n",
    "recipe_ingredients = data[\"@graph\"][7][\"recipeIngredient\"]\n",
    "\n",
    "# Print the list of ingredients\n",
    "for ingredient in recipe_ingredients:\n",
    "    print(ingredient)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's also print out the instructions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "recipe_instructions= data[\"@graph\"][7][\"recipeInstructions\"]\n",
    "#the instructions list contains dictionaries as elements, take a look at how the list is organized\n",
    "for step in recipe_instructions:\n",
    "    print(step[\"text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "Websites are going to be structured differently. Look at the following JSON-DL snippet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "json_example = {\n",
    "    \"title\": \"The anarchist cookbook\",\n",
    "    \"recipeInstructions\": \"<ol class=\\\"recipeSteps\\\"><li>Cook the linguine according to the packet instructions. </li><li>Meanwhile, carefully crack the eggs into a small bowl and beat them with a fork. Season with a little black pepper, then stir in the ricotta finely grate in most of the lemon zest. </li><li>When the pasta has 3 minutes left, add the peas. Reserve a little cooking water, then drain the linguine and peas, and return to the pan. </li><li>Stir in the egg mixture and spinach with a wooden spoon â€“ they'll cook gently in the residual heat. Add a little pasta water to loosen, if needed. </li><li>Share between bowls and serve with a green salad.</li></ol>\",\n",
    "    \"ingredients\": [\"a lot of effort\", \"the right mindset\"]\n",
    "}\n",
    "\n",
    "recipe_instructions = json_example[\"recipeInstructions\"]\n",
    "example_soup = BeautifulSoup(recipe_instructions, 'html.parser')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get a nice and clean list of the instructions, step by step, we can use the `.find` method to get the first `\"ol\"` element,  with attribute `\"class..\"`, and then use `.find_all` to get all list elements in there.  \n",
    "Lastly, we can `strip` the list items to obtain the instructions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "list_items = example_soup.find('ol', class_='recipeSteps').find_all('li')\n",
    "instructions = [item.get_text(strip=True) for item in list_items]\n",
    "print(instructions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RYb6WtXYeMJ6"
   },
   "source": [
    "### Task 2.1\n",
    "Now it's your turn. Create a function that can scrape any recipe webpage from the same website (other websites will have different class tags). \n",
    "\n",
    "Make sure to:\n",
    "\n",
    "- Return itemized content (e.g. ingredients) in a list. (You may want to use a list comprehension here)\n",
    "- Not all items have been cleaned of their html markdown (see variables ```ingredients``` vs. ```instructions_unparsed```). Make sure to return a list with human readable content (i.e. by using the ```.text``` attribute).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Here you can see the solution for our example website\n",
    "\n",
    "URL = \"https://www.acouplecooks.com/spring-vegetarian-spaghetti-carbonara\"\n",
    "\n",
    "def parse_website(url):\n",
    "    # Create a CloudScraper object\n",
    "    scraper = cloudscraper.create_scraper()\n",
    "\n",
    "    # Use the CloudScraper object to fetch the HTML content\n",
    "    response = scraper.get(URL)\n",
    "\n",
    "    # Parse the HTML content with BeautifulSoup\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    \n",
    "    #Get the title\n",
    "    title_unparsed = soup.select_one(\"title\")\n",
    "    fn = title_unparsed.text\n",
    "    \n",
    "    json_ld_script = soup.find(\"script\", {\"class\": \"yoast-schema-graph\"})\n",
    "\n",
    "    # Extract the content of the script tag\n",
    "    script_content = json_ld_script.string\n",
    "\n",
    "    # Load the JSON data from the script content\n",
    "    data = json.loads(script_content)\n",
    "\n",
    "    # Access the \"recipeIngredient\" list\n",
    "    recipe_ingredients = data[\"@graph\"][7][\"recipeIngredient\"]\n",
    "    \n",
    "    ingredients = [ingredient for ingredient in recipe_ingredients]\n",
    "    \n",
    "    #Access the instructions\n",
    "    recipe_instructions= data[\"@graph\"][7][\"recipeInstructions\"]\n",
    "    #the instructions list contains dictionaries as elements, take a look at how the list is organized\n",
    "    instructions = [step[\"text\"] for step in recipe_instructions]\n",
    "\n",
    "    return {'name': fn,\n",
    "            'ingredients': ingredients,\n",
    "            'instructions': instructions,\n",
    "            }\n",
    "    \n",
    "recipe = parse_website(URL)\n",
    "print (recipe)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Implement your function in the cell bellow:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "id": "UQu9ecLEeMJ6",
    "outputId": "a8aa0e14-a8fb-4279-cf32-8dca97ab3412"
   },
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "import cloudscraper\n",
    "import json\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Pass in a URL containing hRecipe, such as\n",
    "# https://www.jamieoliver.com/recipes/pasta-recipes/veggie-carbonara/\n",
    "\n",
    "URL = #YOUR RECIPE HERE\n",
    "\n",
    "# Parse out some of the pertinent information for a recipe.\n",
    "# See http://microformats.org/wiki/hrecipe.\n",
    "\n",
    "#Solution for jamie oliver\n",
    "def parse_website(url):\n",
    "\n",
    "    return {\n",
    "            'name': fn,\n",
    "            'ingredients': ingredients,\n",
    "            'instructions': instructions,\n",
    "            }\n",
    "    \n",
    "recipe = parse_website(URL)\n",
    "print (recipe)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ccURluAIeMJ8"
   },
   "source": [
    "#### How Can We Extract Information from Multiple Websites?\n",
    "\n",
    "**The answer:** microformats.\n",
    "\n",
    "Instead of manually extracting information from microformats like `schema.org` or `hRecipe`, you can use a package called `scrape-schema-recipe`.\n",
    "\n",
    "Feel free to give it a try!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EBY-y_GreMJ8"
   },
   "source": [
    "### Task 2.2\n",
    "`hRecipe` is a microformat specifically created for recipes. \n",
    "\n",
    "**For this task**, you have to compare different dessert recipe ingredients. (For inspiration, you can look back at the exercises you did in Hands-on session 1 where you compared different sets of tweets.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import scrape_schema_recipe\n",
    "\n",
    "# TODO."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "n-J8fiLbeMJ9"
   },
   "source": [
    "## Exercise 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7XBeqJHVeMJ9"
   },
   "source": [
    "`Schema.org` is one of the most widely used annotations formats. It is a multipurpose template that has been created by a consortium consisting of Yahoo!, Google and Microsoft. It can describe `entities`, `events`, `products` etc. \n",
    "\n",
    "Check out the vocabulary specs on [schema.org][s].\n",
    "\n",
    "[s]: https://schema.org/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fiw8JClyeMJ-"
   },
   "source": [
    "### Task 3 - Parsing schema.org microdata\n",
    "\n",
    "To parse this data, you need to install the `rdflib-microdata` package. (which you have done in one of the previous steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 153
    },
    "id": "X2zr3fOOeMJ-",
    "outputId": "d123f981-d73f-470f-b5e9-8735819f894b"
   },
   "outputs": [],
   "source": [
    "from rdflib import Graph\n",
    "\n",
    "# Source: https://www.youtube.com/watch?v=sCU214rbRZ0\n",
    "# Pass in a URL containing Schema.org microformats\n",
    "URL = \"http://dbpedia.org/resource/Micheal_Jackson\"\n",
    "\n",
    "# Initialize a graph\n",
    "g = Graph()\n",
    "\n",
    "# Parse in an RDF file graph dbpedia\n",
    "result = g.parse(location=URL)\n",
    "\n",
    "# Loop through first 10 triples in the graph\n",
    "for index, (sub, pred, obj) in enumerate(g):\n",
    "    print(sub, pred, obj)\n",
    "    if index == 10:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "hrQ2EuY5JAn1",
    "outputId": "eba60ebb-7ac5-4451-c16e-3f68e66af7f3"
   },
   "outputs": [],
   "source": [
    "# Print the size of the Graph\n",
    "print(f'Graph has {len(g)} facts')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 323
    },
    "id": "IAO1JllwJMqO",
    "outputId": "08f5e32d-d1a6-4a30-878a-ce7b768a8811"
   },
   "outputs": [],
   "source": [
    "# Print out the entire Graph in the RDF Turtle format\n",
    "print(g.serialize(format='ttl'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dzbynasAeMKA"
   },
   "source": [
    "### Task 3.1 \n",
    "Compare the [`schema.org`][s] information about a band (you can choosy any) on [`Last.fm`][l] to the `Facebook Open Graph` information about the **same band** on `Facebook`. \n",
    "- What are the differences? \n",
    "- Which format do you think offers better interoperability?\n",
    "\n",
    "Be sure to refer to the **Microformat** specifications.\n",
    "\n",
    "[l]: https://www.last.fm/\n",
    "[s]: https://schema.org/\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer:** (add your answer here)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Nocs4YDPeMKB"
   },
   "source": [
    "### Task 3.2\n",
    "Explore the various microformats at http://microformats.org/ and compare the output of the exercises with the output of http://microformats.org/. \n",
    "\n",
    "Think about possible microformats you want to support in your final assignment and read up on how to parse them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer:** (add your answer here)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Hands-on_2_microformats.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
